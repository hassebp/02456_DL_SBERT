{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook enables the user to train their own model using our pipeline or load our already trained model.\n",
    "## The notebook is divided into two parts.\n",
    "### First part\n",
    "Download the necessary data and scripts to run training\n",
    "### Second part\n",
    "Load in the pretrained model and run evaluation with some example queries we have used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-21 19:42:46 - Create new SBERT model\n",
      "2023-12-21 19:42:47 - Loading validation: validation corpus\n",
      "2023-12-21 19:42:47 - Loading validation: validation queries\n",
      "2023-12-21 19:42:47 - Loading validation: validation keywords\n",
      "2023-12-21 19:42:47 - Loading trianing: training corpus\n",
      "2023-12-21 19:42:47 - Loading training: training queries\n",
      "2023-12-21 19:42:47 - Loading training: keywords corpus\n",
      "2023-12-21 19:42:47 - Read hard negatives train file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\LoggingHandler.py\", line 10, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 529, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 518, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 424, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 766, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Temp\\ipykernel_18616\\1427736759.py\", line 198, in <module>\n",
      "    logging.info(\"Using negatives from the following systems:\", negs_to_use)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 2097, in info\n",
      "    root.info(msg, *args, **kwargs)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\LoggingHandler.py\", line 16, in emit\n",
      "    self.handleError(record)\n",
      "Message: 'Using negatives from the following systems:'\n",
      "Arguments: (['jaccard_custom'],)\n",
      "19432it [00:00, 60147.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-21 19:42:47 - Train queries: 19409\n",
      "2023-12-21 19:42:47 - Read hard negatives valid file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\LoggingHandler.py\", line 10, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 927, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 663, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 367, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1077, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 529, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 518, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 424, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 766, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Temp\\ipykernel_18616\\1427736759.py\", line 292, in <module>\n",
      "    logging.info(\"Using negatives from the following systems:\", negs_to_use)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 2097, in info\n",
      "    root.info(msg, *args, **kwargs)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"C:\\Users\\hasse\\AppData\\Local\\Programs\\Python\\Python39\\lib\\logging\\__init__.py\", line 952, in handle\n",
      "    self.emit(record)\n",
      "  File \"c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\LoggingHandler.py\", line 16, in emit\n",
      "    self.handleError(record)\n",
      "Message: 'Using negatives from the following systems:'\n",
      "Arguments: (['jaccard_custom'],)\n",
      "4164it [00:00, 99118.53it/s]\n",
      "Iteration:   0%|          | 0/303 [00:19<?, ?it/s]\n",
      "Epoch:   0%|          | 0/50 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 366\u001b[0m\n\u001b[0;32m    363\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mTripletLoss(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 366\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m          \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTripletEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_anchor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegatives\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneg_sentence\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#evaluation_steps=args.eval_steps,\u001b[39;49;00m\n\u001b[0;32m    369\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m          \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m          \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#checkpoint_path=model_save_path,\u001b[39;49;00m\n\u001b[0;32m    373\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m#checkpoint_save_steps=500,\u001b[39;49;00m\n\u001b[0;32m    374\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m          \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m    376\u001b[0m \u001b[43m          \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Train latest model\u001b[39;00m\n\u001b[0;32m    379\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_save_path)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:710\u001b[0m, in \u001b[0;36mSentenceTransformer.fit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_amp:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m--> 710\u001b[0m         loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    712\u001b[0m     scale_before_step \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[0;32m    713\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss_value)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\losses\\TripletLoss.py:61\u001b[0m, in \u001b[0;36mTripletLoss.forward\u001b[1;34m(self, sentence_features, labels)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[1;32m---> 61\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[0;32m     63\u001b[0m     rep_anchor, rep_pos, rep_neg \u001b[38;5;241m=\u001b[39m reps\n\u001b[0;32m     64\u001b[0m     distance_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric(rep_anchor, rep_pos)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\losses\\TripletLoss.py:61\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor):\n\u001b[1;32m---> 61\u001b[0m     reps \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence_feature\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[0;32m     63\u001b[0m     rep_anchor, rep_pos, rep_neg \u001b[38;5;241m=\u001b[39m reps\n\u001b[0;32m     64\u001b[0m     distance_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_metric(rep_anchor, rep_pos)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:359\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, evaluation, losses, InputExample\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import argparse\n",
    "import torch, numpy\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "#### /print debug information to stdout\n",
    "\n",
    "# Used when not in jupyter shitbook\n",
    "\n",
    "\"\"\"parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_batch_size\", default=64, type=int)\n",
    "parser.add_argument(\"--max_seq_length\", default=300, type=int)\n",
    "parser.add_argument(\"--model_name\", required=True)\n",
    "parser.add_argument(\"--max_passages\", default=0, type=int)\n",
    "parser.add_argument(\"--epochs\", default=30, type=int)\n",
    "parser.add_argument(\"--pooling\", default=\"mean\")\n",
    "parser.add_argument(\"--negs_to_use\", default=None, help=\"From which systems should negatives be used? Multiple systems seperated by comma. None = all\")\n",
    "parser.add_argument(\"--warmup_steps\", default=500, type=int)\n",
    "parser.add_argument(\"--lr\", default=2e-5, type=float)\n",
    "parser.add_argument(\"--num_negs_per_system\", default=5, type=int)\n",
    "parser.add_argument(\"--use_pre_trained_model\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--use_all_queries\", default=False, action=\"store_true\")\n",
    "parser.add_argument(\"--eval_steps\", default=500, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "logging.info(str(args))\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# The  model we want to fine-tune\n",
    "train_batch_size = 64 #args.train_batch_size          #Increasing the train batch size improves the model performance, but requires more GPU memory\n",
    "model_name = 'bert-base-uncased' #args.model_name\n",
    "max_passages = 0 #args.max_passages\n",
    "max_seq_length = 300 #args.max_seq_length            #Max length for passages. Increasing it, requires more GPU memory\n",
    "\n",
    "num_negs_per_system = 5 #args.num_negs_per_system  # We used different systems to mine hard negatives. Number of hard negatives to add from each system\n",
    "num_epochs = 50 #args.epochs         # Number of epochs we want to train\n",
    "use_pre_trained_model = False\n",
    "pooling = \"mean\"\n",
    "warmup_steps = 500 \n",
    "lr = 2e-5\n",
    "\n",
    "# Load our embedding model\n",
    "if use_pre_trained_model:\n",
    "    logging.info(\"use pretrained SBERT model\")\n",
    "    model = SentenceTransformer(model_name, device='cuda')\n",
    "    model.max_seq_length = max_seq_length\n",
    "else:\n",
    "    logging.info(\"Create new SBERT model\")\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling)\n",
    "    model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device='cuda')\n",
    "\n",
    "model_save_path = f'output/train_bi-encoder-margin_mse-{model_name.replace(\"/\", \"-\")}-batch_size_{train_batch_size}-{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "\n",
    "\n",
    "# Write self to path\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "train_script_path = os.path.join(model_save_path, 'train_script.py')\n",
    "#copyfile(__file__, train_script_path)\n",
    "#with open(train_script_path, 'a') as fOut:\n",
    "   # fOut.write(\"\\n\\n# Script was called via:\\n#python \" + \" \".join(sys.argv))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA VALIDATION LOADING\n",
    "\"\"\"\n",
    "valid_data_folder = 'data/valid'\n",
    "\n",
    "#### Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
    "val_corpus = {}         #dict in the format: passage_id -> passage. Stores all existent passages\n",
    "val_corpus_path = os.path.join(valid_data_folder, 'valid_corpus.csv')\n",
    "\n",
    "logging.info(\"Loading validation: validation corpus\")\n",
    "with open(val_corpus_path, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, passage = line.strip().split(\";\")\n",
    "        qid = int(qid)\n",
    "        val_corpus[qid] = passage\n",
    "        \n",
    "#### Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
    "val_queries = {}         #dict in the format: passage_id -> passage. Stores all existent passages\n",
    "val_queries_path =  os.path.join(valid_data_folder, 'valid_queries.csv')\n",
    "\n",
    "logging.info(\"Loading validation: validation queries\")\n",
    "with open(val_queries_path, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        try:\n",
    "            qid, passage, q = line.strip().split(\";\")\n",
    "            qid = int(qid)\n",
    "            val_queries[qid] = passage\n",
    "        except:\n",
    "            continue\n",
    "      \n",
    "\n",
    "val_keywords = {}\n",
    "keywords_filepath =  os.path.join(valid_data_folder, 'valid_keywords.csv')\n",
    "logging.info(\"Loading validation: validation keywords\")\n",
    "with open(keywords_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        row = line.strip().split(\";\")\n",
    "        pid, qid, keywordss = row[0], row[1], row[2:]\n",
    "        qid = int(qid)\n",
    "        val_keywords[qid] = keywordss\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DATA TRAINING LOADING\n",
    "\"\"\"\n",
    "\n",
    "train_data_folder = 'data/train'\n",
    "\n",
    "#### Read the corpus files, that contain all the passages. Store them in the corpus dict\n",
    "train_corpus = {}         #dict in the format: passage_id -> passage. Stores all existent passages\n",
    "collection_filepath = os.path.join(train_data_folder, 'train_corpus.csv')\n",
    "\n",
    "logging.info(\"Loading trianing: training corpus\")\n",
    "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\";\")\n",
    "        pid = int(pid)\n",
    "        train_corpus[pid] = passage\n",
    "\n",
    "\n",
    "### Read the train queries, store in queries dict\n",
    "queries = {}        #dict in the format: query_id -> query. Stores all training queries\n",
    "queries_filepath = os.path.join(train_data_folder, 'train_queries.csv')\n",
    "logging.info(\"Loading training: training queries\")\n",
    "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        try:\n",
    "            qid, passage, q = line.strip().split(\";\")\n",
    "            qid = int(qid)\n",
    "            queries[qid] = passage\n",
    "        except:\n",
    "            continue\n",
    "       \n",
    "        \n",
    "\n",
    "train_keywords = {}\n",
    "keywords_filepath = os.path.join(train_data_folder, 'train_keywords.csv')\n",
    "logging.info(\"Loading training: keywords corpus\")\n",
    "with open(keywords_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        row = line.strip().split(\";\")\n",
    "        pid, qid, keywordss = row[0], row[1], row[2:]\n",
    "        qid = int(qid)\n",
    "        train_keywords[qid] = keywordss\n",
    "\n",
    "    \n",
    "train_queries = {}\n",
    "hard_negatives_filepath = os.path.join('data', 'hard_negs.jsonl.gz')\n",
    "logging.info(\"Read hard negatives train file\")\n",
    "\n",
    "train_keys = queries.keys()\n",
    "use_all_queries = False # These two are normally sat in argparser\n",
    "negs_to_use = None\n",
    "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        if max_passages > 0 and len(train_queries) >= max_passages:\n",
    "            break\n",
    "        \n",
    "        data = json.loads(line)\n",
    "        \n",
    "        if not data['qid'] in queries:\n",
    "            continue\n",
    "        \n",
    "        #Get the positive passage ids\n",
    "        pos_pids = data['pos']\n",
    "        \n",
    "        #Get the hard negatives\n",
    "        neg_pids = set()\n",
    "        if negs_to_use is None:\n",
    "            if negs_to_use is not None:    #Use specific system for negatives\n",
    "                negs_to_use = negs_to_use.split(\",\")\n",
    "            else:   #Use all systems\n",
    "                negs_to_use = list(data['neg'].keys())\n",
    "            logging.info(\"Using negatives from the following systems:\", negs_to_use)\n",
    "\n",
    "        for system_name in negs_to_use:\n",
    "            if system_name not in data['neg']:\n",
    "                continue\n",
    "\n",
    "            system_negs = data['neg'][system_name]\n",
    "            negs_added = 0\n",
    "            for pid in system_negs:\n",
    "                if pid not in neg_pids:\n",
    "                    neg_pids.add(pid)\n",
    "                    negs_added += 1\n",
    "                    if negs_added >= num_negs_per_system:\n",
    "                        break\n",
    "                    \n",
    "        if use_all_queries or (len(pos_pids) > 0 and len(neg_pids) > 0):   \n",
    "            train_queries[data['qid']] = {'qid': data['qid'], 'query': queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n",
    "                \n",
    "\n",
    "\"\"\"\n",
    "SETUP DATASET\n",
    "\"\"\"\n",
    "\n",
    "logging.info(\"Train queries: {}\".format(len(train_queries)))\n",
    "\n",
    "# We create a custom MSMARCO dataset that returns triplets (query, positive, negative)\n",
    "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
    "class MSMARCODataset(Dataset):\n",
    "    def __init__(self, queries, corpus, keywords):\n",
    "        self.queries = queries\n",
    "        self.queries_ids = list(queries.keys())\n",
    "        self.keywords = keywords\n",
    "        self.corpus = corpus\n",
    "        \n",
    "        for qid in self.queries:\n",
    "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
    "            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n",
    "            random.shuffle(self.queries[qid]['neg'])\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        query = self.queries[self.queries_ids[item]]\n",
    "        query_text = query['query']\n",
    "        qid = query['qid']\n",
    "        keywords = ' '.join(self.keywords[qid])\n",
    "        query_text = query_text + keywords.replace('\"',' ')\n",
    "        if len(query['pos']) > 0:\n",
    "            pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
    "            pos_text = self.corpus[pos_id]\n",
    "            query['pos'].append(pos_id)\n",
    "        else:   #We only have negatives, use two negs\n",
    "            pos_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "            pos_text = self.corpus[pos_id]\n",
    "            query['neg'].append(pos_id)\n",
    "\n",
    "        #Get a negative passage\n",
    "        neg_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "        neg_text = self.corpus[neg_id]\n",
    "        query['neg'].append(neg_id)\n",
    "       \n",
    "        return InputExample(texts=[query_text, pos_text, neg_text], label=1)\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "SETUP VALIDATION\n",
    "\"\"\"\n",
    "valid_queries = {}\n",
    "hard_negatives_filepath = os.path.join('data', 'valid_hard_negs.jsonl.gz')\n",
    "logging.info(\"Read hard negatives valid file\")\n",
    "use_all_queries = False\n",
    "negs_to_use = None\n",
    "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        if max_passages > 0 and len(valid_queries) >= max_passages:\n",
    "            break\n",
    "        \n",
    "        data = json.loads(line)\n",
    "        \n",
    "        if not data['qid'] in val_queries:\n",
    "            continue\n",
    "        \n",
    "        #Get the positive passage ids\n",
    "        pos_pids = data['pos']\n",
    "        \n",
    "        #Get the hard negatives\n",
    "        neg_pids = set()\n",
    "        if negs_to_use is None:\n",
    "            if negs_to_use is not None:    #Use specific system for negatives\n",
    "                negs_to_use = negs_to_use.split(\",\")\n",
    "            else:   #Use all systems\n",
    "                negs_to_use = list(data['neg'].keys())\n",
    "            logging.info(\"Using negatives from the following systems:\", negs_to_use)\n",
    "\n",
    "        for system_name in negs_to_use:\n",
    "            if system_name not in data['neg']:\n",
    "                continue\n",
    "\n",
    "            system_negs = data['neg'][system_name]\n",
    "            negs_added = 0\n",
    "            for pid in system_negs:\n",
    "                if pid not in neg_pids:\n",
    "                    neg_pids.add(pid)\n",
    "                    negs_added += 1\n",
    "                    if negs_added >= num_negs_per_system:\n",
    "                        break\n",
    "                    \n",
    "        if use_all_queries or (len(pos_pids) > 0 and len(neg_pids) > 0):\n",
    "            valid_queries[data['qid']] = {'qid': data['qid'], 'query': val_queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n",
    "                \n",
    "\n",
    "\n",
    "class GenerateValidationTriplets():\n",
    "    def __init__(self, validation_queries, validation_corpus, validation_keywords):\n",
    "        self.queries = validation_queries\n",
    "        self.queries_ids = list(self.queries.keys())\n",
    "        self.keywords = validation_keywords\n",
    "        self.corpus = validation_corpus\n",
    "   \n",
    "        \n",
    "        for qid in self.queries:\n",
    "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
    "            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n",
    "            random.shuffle(self.queries[qid]['neg'])\n",
    "            \n",
    "    def getdata(self):\n",
    "        val_anchor = []\n",
    "        pos_sentence = []\n",
    "        neg_sentence = []\n",
    "        for item in range(len(self.queries)):\n",
    "            query = self.queries[self.queries_ids[item]]\n",
    "            query_text = query['query']\n",
    "            qid = query['qid']\n",
    "            keywords = ' '.join(self.keywords[qid])\n",
    "            query_text = query_text + keywords.replace('\"',' ')\n",
    "            if len(query['pos']) > 0:\n",
    "                pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
    "                pos_text = self.corpus[pos_id]\n",
    "                query['pos'].append(pos_id)\n",
    "            else:   #We only have negatives, use two negs\n",
    "                pos_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "                pos_text = self.corpus[pos_id]\n",
    "                query['neg'].append(pos_id)\n",
    "\n",
    "            #Get a negative passage\n",
    "            neg_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "            neg_text = self.corpus[neg_id]\n",
    "            query['neg'].append(neg_id)\n",
    "            \n",
    "            val_anchor.append(query_text)\n",
    "            pos_sentence.append(pos_text)\n",
    "            neg_sentence.append(neg_text)\n",
    "        \n",
    "        return val_anchor, pos_sentence, neg_sentence\n",
    "    \n",
    "    \n",
    "val_anchor, pos_sentence, neg_sentence = GenerateValidationTriplets(valid_queries, val_corpus, val_keywords).getdata()\n",
    "\"\"\"\n",
    "TRAIN MODEL\n",
    "\"\"\"\n",
    "# For training the SentenceTransformer model, we need a dataset, a dataloader, and a loss used for training.\n",
    "train_dataset = MSMARCODataset(queries=train_queries, corpus=train_corpus, keywords=train_keywords)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size, drop_last=True)\n",
    "train_loss = losses.TripletLoss(model=model)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluation.TripletEvaluator(anchors=val_anchor, positives=pos_sentence, negatives=neg_sentence),\n",
    "          #evaluation_steps=args.eval_steps,\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          use_amp=True,\n",
    "          #checkpoint_path=model_save_path,\n",
    "          #checkpoint_save_steps=500,\n",
    "          optimizer_params = {'lr': lr},\n",
    "          output_path='test/'\n",
    "          )\n",
    "\n",
    "# Train latest model\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model using this url: https://dtudk-my.sharepoint.com/:f:/g/personal/hbype_dtu_dk/EgskgXe0X1BKnLEbKdJ7Iv4BFI7jySRs-VZNZn76340aGQ?e=FrWpfi\n",
    "\n",
    "If you dont have access for some weird reason, write an email to hbype@space.dtu.dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hasse\\Skrivebord\\02456_DL_SBERT\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from sentence_transformers import SentenceTransformer\n",
    "from postprocessing import embed\n",
    "import torch\n",
    "import numpy\n",
    "from main import search_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding sentences: 100%|| 4164/4164 [01:10<00:00, 59.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 most similar articles in the corpus:\n",
      "1187\n",
      "Title: Surfing Injuries: A Review for the Orthopaedic Surgeon \n",
      " (Score: 0.2391) \n",
      " url: https://findit.dtu.dk/en/catalog/606d9336d9001d01c01386f8 \n",
      "\n",
      "824\n",
      "Title: Clinical and Histologic Characterization of Co-infection with Astrovirus and Goose Parvovirus in Goslings \n",
      " (Score: 0.2286) \n",
      " url: https://findit.dtu.dk/en/catalog/5e01ffded9001d0457169e29 \n",
      "\n",
      "2859\n",
      "Title: Metagenomic analysis of oral microbiota among oral cancer patients and tobacco chewers in Rajasthan, India \n",
      " (Score: 0.2171) \n",
      " url: https://findit.dtu.dk/en/catalog/64ac9dc7bcb69d910bcfd479 \n",
      "\n",
      "1208\n",
      "Title: Eumycetoma, A Neglected Tropical Disease in the United States \n",
      " (Score: 0.2122) \n",
      " url: https://findit.dtu.dk/en/catalog/623477dfbc81d1c155980319 \n",
      "\n",
      "4060\n",
      "Title: Head injuries in professional football (soccer): Results of video analysis verified by an accident insurance registry \n",
      " (Score: 0.2091) \n",
      " url: https://findit.dtu.dk/en/catalog/61165320d9001d01b32ff15d \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus_path = 'data/test/test_corpus.csv' ### You can change to train/train_corpus.csv or data/corpus.csv to try with the train set or the whole dataset - remember to encode!\n",
    "model_path = 'model' ### Insert model \n",
    "corpus_embeddings_path = os.path.join(os.getcwd(), 'data/embeddings')\n",
    "corpus_path = os.path.join(os.getcwd(), corpus_path)\n",
    "model = SentenceTransformer(model_path)\n",
    "corpus_ids = []\n",
    "with open(corpus_path, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\";\")\n",
    "        corpus_ids.append(pid)\n",
    "        \n",
    "if not os.path.exists(corpus_embeddings_path + '.npy'):\n",
    "    embed(corpus_path, model, corpus_embeddings_path)\n",
    "\n",
    "corpus_embeddings = torch.from_numpy(numpy.load('data/embeddings.npy'))\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Enter your research query (or 'exit' to quit): \")\n",
    "    if user_query.lower() == 'exit':\n",
    "        break\n",
    "    search_articles(user_query, model, corpus_embeddings,corpus_ids)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
